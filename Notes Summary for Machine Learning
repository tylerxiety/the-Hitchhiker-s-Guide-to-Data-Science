<A summary for my notes of Andrew Ng's Machine Learning on Coursera >
<table>
  <tr><td><div align="center"><b>Models</b></div></td>
   <td><div align="center"><b>Topics and Techniques Covered</b></div></td></tr>
  <tr><td colspan="2"><div align="center"><b>Supervised Learning Models</b></div></td></tr>
  <tr><td>1. <b>Linear Regression</b> (Week 1&2):<br>Predicting housing prices.</td>
   <td><ul><li>One variable and multi-variable algorithms</li><li>Learning method: gradient descent vs. normal equation</li><li>Regularization</li><li>Solving non-invertibility and removing linearly dependency</li></ul></td></tr>
  <tr><td>2. <b>Logistic Regression</b> (Week 3): Classifying emails.</td>
   <td><ul><li>Binary classification and multi-class classification algorithms</li><li>Leaning method: gradient descent and advanced optimization algorithms (e.g. Conjugate Gradient, BFGS, L-BFGS)</li></ul></td></tr>
  <tr><td>3. <b>Neural Networks</b> (Week 4&5)</td>
   <td><ul><li>Non-linear regression and classification algorithms</li><li>Activation function, network architectures</li><li>Forward propagation and backpropagation</li><li>Unrolling parameters, gradient checking, random initialization</li></ul></td></tr>
  <tr><td>4. <b>Support Vector Machines (SVMs)</b> (Week7)</td>
   <td><ul><li>Linear classification and non-linear classification with kernels</li><li>Choice of parameters, choice of kernels/ similarity functions</li><li>Logistic regression vs. SVMs</li></ul></td></tr>
  <tr><td>5. <b>Online Learning algorithm</b> (Week 10)</td>
   <td><ul><li>Continuous stream of data</li><li>Stochastic gradient descent</li></ul></td></tr>
  <tr><td colspan="2"><div align="center"><b>Unsupervised Learning Models</b></div></td></tr>
  <tr><td>6. <b>K-means Clustering</b> (Week 8)</td>
   <td><ul><li>Random initialization</li><li>Choosing the value of K</li><li>Non-separated clusters</li></ul></td></tr>
  <tr><td>7. <b>Principle Component Analysis (PCA)</b> (Week 8)</td>
   <td><ul><li>Application of data compression and data visualization</li><li>PCA vs. linear regression</li><li>Reconstruction data after PCA</li></ul></td></tr>
  <tr><td>8. <b>Anomaly Detection Algorithm</b> (Week 9)</td>
   <td><ul><li>Density estimation, multivariate normal distribution</li><li>Choice of features</li><li>Anomaly detection vs. supervised learning</li></ul></td></tr>
  <tr><td>9. <b>Recommender Systems</b> (Week 9)</td><td><ul><li>Content-based recommender algorithm and collaborative filtering algorithm</li><li>Vectorization, mean normalization</li></ul></td></tr>
  <tr><td colspan="2"><div align="center"><b>General Advice and Techniques</b> (Week 1, 3, 6&10)</div></td></tr>
  <tr><td colspan="2"><ul><li>Model selection, learning rate</li><li>Diagnosing bias (underfitting) vs. variance (overfitting)</li><li>Error analysis and metrics</li><li>Feature Scaling and mean normalization, feature engineering</li><li>Large scale machine learning with big data: choice of batch gradient descent, stochastic gradient descent and mini-batch gradient descent</li><li>Data parallelism and Map Reduce</li></ul></td></tr>
</table>
